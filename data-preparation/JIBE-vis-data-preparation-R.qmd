---
title: "JIBE Melbourne Mode Choice Preparation and Analysis"
format: gfm
execute:
  # no data is to be included in this document, unless intentionally over-ridden
  output: false 
knitr:
  opts_chunk: 
    collapse: true
---

# JIBE-Vis data preparation

The JIBE-Vis project aims to make the [Joining Impact models of transport
with spatial measures of the Built Environment (JIBE) project](https://jibeproject.com) modelling outputs accessible and useful for stakeholders, both in Melbourne and Manchester.  More broadly, the proposed platform (https://transporthealthimpacts.org) is intended to serve as an ongoing outlet for data stories illustrating the health impacts of transport planning scenarios.

This [Quarto](https://quarto.org/) markdown document will be used to record additional data processing undertaken using JIBE modelling outputs and additional external data for inclusion in the [Transport Health Impacts](https://transporthealthimpacts.org) data platform.


## Background

Through the JIBE project (Joining Impact models of transport with spatial measures of the Built Environment) and the associated [AToM project](https://doi.org/10.1080/15472450.2024.2372894) ( Activity-based and agent-based Transport model of Melbourne), we have developed agent-based transport simulation models (ABMs) capable of depicting complex urban systems. These ABMs model how street-level built environment exposures influence behaviour, accessibility and health with high spatial and demographic granularity. By forecasting travel itineraries, behaviours, exposures, and health for a synthetic population of individuals, these ABMs allow us to simulate scenarios of interest to health and transport planners. However, the complexity of the models and their extensive, detailed outputs can be a barrier to effective knowledge translation and therefore impact.

This web site provides illustrative examples of potential functionality that we could implement in an interactive tool to make transport and health modelling results from JIBE and similar projects accessible and useful. Through our engagement with stakeholders, we will incorporate and test new functionality that can help meet their needs and achieve this goal. The website is being developed as open source software on [GitHub](https://github.com/jibeproject/jibe-vis).

The approach to complex systems modelling undertaken in the JIBE project is illustrated in the following diagram:
![JIBE model diagram](https://github.com/jibeproject/jibe-vis/blob/main/diagrams/jibe-model-diagram.png?raw=true)
 
### Aims
We plan to engage government and advocacy stakeholders and researchers to co-develop an interactive platform with two related aims:

1. To make complex urban systems modelling evidence accessible and useful for informing healthy transport planning policy and localised infrastructure interventions; and
2. Support visualising the impacts of modelled transportation scenarios.

We plan to publish the methods and visualisation platform developed through this work as open source code that can be adapted by other researchers and practitioners for new settings for translation of research evidence into practice.
 
## JIBE outputs

The JIBE outputs are stored in a shared sharepoint/Teams folder ([JIBE working group](https://rmiteduau.sharepoint.com/:f:/r/sites/JIBEworkinggroup/Shared%20Documents/General?csf=1&web=1&e=xxdKd0)) that contains the following sub-folders:

- documentation
  - a spreadsheet for documenting outputs and their documentation
- manchester
  - documentation and outputs relating to the Manchester modelling components:
    - accessibility
    - airPollutant
    - health
    - MATSim
    - network
    - noise
    - physicalActivity
    - skims
    - synPop
- melbourne
  - documentation and outputs relating to the Melbourne modelling components
    - 20min_interventions
    - addresses
    - cycling_intervention
    - dem
    - freight
    - gtfs
    - health
    - injuries
    - network
    - noise
    - osm
    - poi
    - regions
    - synthetic_population
- visualisation
  - documentation and outputs related to the JIBE visualisation research translation project component, including this notebook and a copy of the Transport Health Impacts website code (a NodeJS typescript website, authored using the React framework).  Because of challenges with syncing and building/rebuilding NodeJS libraries, the actual development is occurring in an offline folder.  This folder contains a copy of the code repository https://github.com/jibeproject/jibe-vis.  The app itself is deployed upon successful code pushes to GitHub using AWS Amplify, at https://transporthealthimpacts.org.
  - This notebook is now being commenced as a refinement of seperate OneNote records, and will also record future planned data preparation drawing on data located in the above mentioned folders.

## Pre-requisites
Code was authored using [R](https://cloud.r-project.org/) 4.4.1 and [Positron 2024.10.0-14](https://github.com/posit-dev/positron/releases/tag/2024.10.0-14) IDE.

No data or data-related outputs will be included in this document or repository. By default, output is set to False. For non-sensitive aspects, e.g. displaying the sessionInfo() after running analysis, this may be over-ridden.

Note that due to limitations of the Postrion IDA (currently in Beta), large code chunks must be split up in order to note exceed R's internal console buffer size.  So, this will be done below (for example, when defining area data).

The following code will help ensure all following code is run from a fresh R instance.
```{r}
rm(list = ls()) # clear memory
```

## General principles

A number of steps must be undertake for data to be included in the Transport Health Impacts (aka JIBE Vis) platform.  Data should be prepared at the appropriate scales required for visualisation with only the relevant variables that will be used.  Reducing the complexity of data in this way will result in lower file sizes and improved performance when streaming data over the internet and processing it on user's computers (which may be mobile phones, laptops or desktop computers).

In general, data which is to be mapped is required in the Protomaps [pmtiles](https://docs.protomaps.com/pmtiles/) format.  These is a vector map tile format, optimised for streaming complex data at a range of spatial scales for use in interactive map visualisations.  These files will be uploaded to the `tiles` folder in the Transport Health Impacts platform's Amazon Web Services (AWS) S3 storage bucket.

In order to get spatial data into the pmtiles format, the software [Tippecanoe](https://github.com/felt/tippecanoe) is used.  Tippecanoe can convert CSV, Geojson, or ideally Flatgeobuf data into vector map tiles in the required format.  Details on the conversion of source data into the required formats will be included in this document.  Tippecanoe should be installed in order to perform this conversion.  The above link contains [instructions](https://github.com/felt/tippecanoe?tab=readme-ov-file#installation) for installing and/or running Tippecanoe locally.  It is easiest on MacOS (`$ brew install tippecanoe`); the code below will assume a local installation has been conducted in this way.  Windows users may find it more convenient running Tippecanoe in a [Docker](https://github.com/felt/tippecanoe?tab=readme-ov-file#docker-image) container, in which case the equivalent Tippecanoe commands listed in this document may be better run directly.

Additional data processing and formatting will be undertaken as required, and documented here.

The following helper function(s) will be used later:

```{r}
spatial_data_to_fgb <- function(spatial_data, output_path, layer = NULL, filter_condition = NULL, linkage = NULL, variables = NULL) {
  if (is.null(layer)) {
    feature_data <- st_read(spatial_data)
  }  else {
    feature_data <- st_read(spatial_data, layer = layer)
  }
  
  # Filter, if defined
  if (!is.null(filter_condition)) {
    feature_data <- feature_data %>%
      filter(!!rlang::parse_expr(filter_condition))
  }
  
  if (!is.null(linkage)) {
    for (source in linkage) {
      lookup <- read_csv(source$source)
      feature_data <- feature_data %>% left_join(lookup[,source$select], by=source$by)
    }
  }

  if (!is.null(variables)) {
    feature_data <- feature_data[,variables]
  }

  # Transform the boundary to EPSG 4326
  feature_data <- st_transform(feature_data, crs = 4326)
  
  # Extract the directory path from the output file path
  output_dir <- dirname(output_path)
  
  # Check if the directory exists, and if not, create it
  if (!dir.exists(output_dir)) {
    dir.create(output_dir, recursive = TRUE)
  }
  
  # Check if file already exists and if it does, delete it (otherwise FGB writing can fail)
  if (file.exists(output_path)) {
    file.remove(output_path)
  }

  # Export the filtered boundary to a FlatGeobuf (fgb) file
  st_write(feature_data, output_path, append = FALSE)
  
  # Confirm the export
  cat("Feature data exported to", output_path, "\n")
  
  return(feature_data)
}
```

## System environment set up
Project dependencies are described in the [`renv.lock`](./renv.lock)
file; see [renv](https://rstudio.github.io/renv/) for more information.
Instructions on installing dependencies using an `renv` lock file are
[here](https://rstudio.github.io/renv/articles/renv.html#installing-packages).

With `renv` installed, dependencies were installed by running:

```
renv::install(c('arrow','conflicted','fastDummies','janitor','knitr','tidyverse','sf','dplyr'))
```

```{r}
library(arrow) # for writing Parquet files
library(conflicted) # package conflict handling https://stackoverflow.com/a/75058976
library(janitor) # data cleaning
library(knitr) # presentation
library(tidyverse) # data handling
library(fastDummies) # binary dummy variable utility
library(sf) # for spatial data handling
library(dplyr)

conflict_prefer("filter", "dplyr")
conflict_prefer("lag", "dplyr")
```

## Data sources

Initialising `data` object which will contain descriptions of the data used for modelling and representing aspects each of Manchester and Melbourne.  A list is initialised and compiled under the respective headers below for relevant area scales used in analysis using official data sources and metadata. Additional statistics are subsequently linked to areas using linkage codes.

It is assumed that the JIBE-Vis data preparation is being constructed within the context of the JIBE working group shared folder structure, and that relative paths are used to link to data stored in this shared folder.  The data preparation script is within a data prepration sub-folder of the JIBE-Vis repository, itself located within the `visualisation` folder. So, paths to city data in other folders will need to step back to reach the root JIBE working group folder.

The following code determines the correct base path to the JIBE working group folder, whether this script is run interactively from the jibe-vis folder or when rendering from the data-preparation folder:

```{r}
# Determine the base path to the JIBE working group folder
# This script is located at: JIBE working group/visualisation/jibe-vis/data-preparation/
# We need to go up 3 directories to reach the JIBE working group folder

# Get the directory containing this script
script_dir <- if (exists("knitr") && !is.null(knitr::current_input())) {
  # When rendering, use the directory of the qmd file
  dirname(knitr::current_input())
} else {
  # When running interactively, use current working directory
  getwd()
}

# Calculate the relative path from script_dir to JIBE working group folder
# The script is in visualisation/jibe-vis/data-preparation, so we need to go up 3 levels
if (basename(script_dir) == "data-preparation") {
  # Script is being run from data-preparation folder (rendering)
  base_path <- "../../../"
} else if (basename(script_dir) == "jibe-vis") {
  # Script is being run from jibe-vis folder (interactive)
  base_path <- "../../"
} else {
  # Try to detect JIBE working group folder by looking for characteristic subdirectories
  check_dir <- script_dir
  levels_up <- 0
  while (levels_up < 5) {
    if (dir.exists(file.path(check_dir, "visualisation")) && 
        dir.exists(file.path(check_dir, "manchester")) && 
        dir.exists(file.path(check_dir, "melbourne"))) {
      base_path <- paste0(rep("../", levels_up), collapse = "")
      if (base_path == "") base_path <- "./"
      break
    }
    check_dir <- dirname(check_dir)
    levels_up <- levels_up + 1
  }
  if (levels_up == 5) {
    stop("Could not determine JIBE working group folder location. Please ensure you're running this script from within the jibe-vis project structure.")
  }
}

cat("Base path to JIBE working group folder:", base_path, "\n")
```

```{r}
# cities <- c("Manchester", "Melbourne", "Munich")
cities <- c("Manchester","Melbourne")
data <- setNames(lapply(cities, function(x) list(areas = list(), linkage = list(), parameters = list())), cities)
data&$Manchester$parameters$name <- "Greater Manchester"
data$Melbourne$parameters$name <- "Greater Melbourne"
data$Manchester$parameters$baseyear <- 2021
data$Melbourne$parameters$baseyear <- 2018
# data$Munich$parameters$baseyear <- 2011
data$Manchester$parameters$currency <- "£"
data$Melbourne$parameters$currency <- "AUD"
# data$Munich$parameters$currency <- "€"
data$Manchester$parameters$scenarios <- list(
  "Reference" = "1_reference", 
  "Safe streets" = "2_safestreet", 
  "Green" = "3_green", 
  "Safe Green Streets (combined)" = "4_both", 
  "Go Dutch" = "5_godutch"
)
data$Melbourne$parameters$scenarios <- list("Reference" = "reference", "Cycling" = "cycling")
data$Manchester$parameters$results_folder <- "manchester/simulationResults/ForPaper/"
data$Melbourne$parameters$results_folder <- "melbourne/simulationResults/SILO - 2025-10-21/"
data$Manchester$parameters$trips <- "/travel_demand_mito/trips.csv"
data$Manchester$parameters$zone <- "Output Areas"
data$Melbourne$parameters$zone <- "SA1"
data$Manchester$parameters$region <- "LSOA"
data$Melbourne$parameters$region <- "SA2"
data$Manchester$parameters$linkage_areas  <- list(
  LSOA = list(
    id = 'LSOA21CD',
    linkage = list(
      lookup = list(
        source = paste0(base_path,data$Manchester$linkage$LSOA2011$source),
        select = data$Manchester$linkage$LSOA2011$variable %>% names(),
        by = 'LSOA21CD'
      ),
      disadvantage = list(
        source = paste0(base_path,data$Manchester$linkage$IMD$source),
        select = data$Manchester$linkage$IMD$variable %>% names(),
        by = 'LSOA11CD'
      )
    )
  ),
  MSOA = list(
    id = 'MSOA21CD',
    linkage = NULL
  ),
  LAD = list(
    id = 'LAD22CD',
    linkage = NULL
  )
),
data$Melbourne$parameters$linkage_areas  <- list(
  SA1 = list(
    id = 'SA1_MAINCODE_2016',
    linkage = list(
      lookup = list(
        source = paste0(base_path,data$Melbourne$linkage$SA1_LGA$source),
        select = data$Melbourne$linkage$SA1_LGA$variable %>% names(),
        by = 'SA1_MAINCODE_2016'
      ),
      disadvantage = list(
        source = paste0(base_path,data$Melbourne$linkage$SEIFA_SA1$source),
        select = data$Melbourne$linkage$SEIFA_SA1$variable %>% names(),
        by = 'SA1_MAINCODE_2016'
      )
    )
  ),
  SA2 = list(
    id = 'SA2_MAINCODE_2016',
    linkage = list(
      disadvantage = list(
        source = paste0(base_path,data$Melbourne$linkage$SEIFA_SA2$source),
        select = data$Melbourne$linkage$SEIFA_SA2$variable %>% names(),
        by = 'SA2_MAINCODE_2016'
      )
    )
  ),
  lGA = list(
    id = 'LGA_CODE_2016',
    linkage = list(
      disadvantage = list(
        source = paste0(base_path,data$Melbourne$linkage$SEIFA_LGA$source),
        select = data$Melbourne$linkage$SEIFA_LGA$variable %>% names(),
        by = 'LGA_CODE_2016'
      )
    )
  )
)
```

All data used should be specified including the following aspects, separated by commas (as demonstrated below):

- source: the path to the data relative to shared project folder (JIBE Working Group)
- description: A brief plain language description of this data
- variable: A list of relevant variables present in this data (using revised names, if optionally renamed, below)
- rename: An optional list mapping old names to new names in the order: old = new
- metadata: A list detailing this dataset's provenance:
  - publisher: For example, 'Office of National Statistics (UK'
  - date_published: For example, '2023'
  - dataset: The official name for this data.
  - url: The URL from which this data may be retrieved.
  - date_accessed: For example, '11 October 2024'
  - licence: The licence governing usage of this data
  - notes: Any relevant notes on usage
- layer: The layer this data is contained with in (option, if relevant, e.g. for a layer in a geopackage)
- filter: An optional filter expression (e.g. "Name == 'Greater Manchester'")
- output: A file path for any derived outputs based on this data

### Manchester areas

#### Output Areas (OA)

```{r}
data$Manchester$areas[["OA"]] = list(
  source="manchester/synPop/sp_2021/OA_2021_MCR.shp",
  description = "Output Areas (2021)",
  variable = list(
    OA21CD = 'Output Area 2021 code',
    LSOA21CD = 'LSOA 2021 code',
    LSOA21NM = 'LSOA 2021 name',
    id = 'Synthetic population zone linkage code for Output Areas'
  ),
  metadata = list(
    publisher = 'Office for National Statistics',
    date_published = '2023',
    dataset = 'Output Areas (December 2021) Boundaries EW BGC (V2',
    url = 'https://geoportal.statistics.gov.uk/datasets/6beafcfd9b9c4c9993a06b6b199d7e6d_0',
    date_accessed = '19 August 2024',
    notes = "This data has been modified with a seprate unique linkage code zone id by the JIBE project team for modelling purposes.",
    licence = 'Open Government Licence (UK)'
  )
)


data$Manchester$areas[["OA_linkage"]] = list(
  source="visualisation/external_data/UK/Office of National Statistics/Output_Area_to_Lower_layer_Super_Output_Area_to_Middle_layer_Super_Output_Area_to_Local_Authority_District_(December_2021)_Lookup_in_England_and_Wales_v3.csv",
  description = "Output Areas linkage codes (LSOA, MSOA, LAD) (2021)",
  variable = list(
  ),
  metadata = list(
    publisher = 'Office for National Statistics',
    date_published = '2024',
    dataset = 'Output Area (2021) to LSOA to MSOA to LAD (December 2021) Exact Fit Lookup in EW (V3) Office for National Statistics Exact Fit Lookup',
    url = 'https://geoportal.statistics.gov.uk/datasets/b9ca90c10aaa4b8d9791e9859a38ca67_0',
    date_accessed = '16 October 2024',
    licence = 'Open Government Licence (UK)'
  )
)
```

#### Lower layer Super Output Areas (LSOA)
```{r}
data$Manchester$areas[["LSOA"]] = list(
  source="visualisation/external_data/UK/Office of National Statistics/Lower_layer_Super_Output_Areas_2021_EW_BGC_V3_4023609225507911834.gpkg",
  description = "Lower layer Super Output Areas (2021)",
  variable = list(
    LSOA21CD = 'LSOA 2021 code',
    LSOA21NM = 'LSOA 2021 name',
    IMD19 = 'Index of Multiple Deprivation (2019)'
  ),
  metadata = list(
    publisher = 'Office for National Statistics',
    date_published = '2024',
    dataset = 'Lower layer Super Output Areas (December 2021) Boundaries EW BGC (V3',
    url = 'https://geoportal.statistics.gov.uk/datasets/d082c4679075463db28bcc8ca2099ade_0',
    date_accessed = '16 October 2024',
    licence = 'Open Government Licence (UK)'
  ),
  output = "visualisation/derived_data/FlatGeobufs/GreaterManchester_LSOA_ONS_2025.fgb"
)
```

#### Middle layer Super Output Areas (MSOA)
```{r}
data$Manchester$areas[["MSOA"]] = list(
  source="visualisation/external_data/UK/Office of National Statistics/MSOA_2021_EW_BGC_V2_6515647442419654873.gpkg",
  description = "Middle layer Super Output Areas (2021)",
  variable = list(
    MSOA21CD = "MSOA 2021 code",
    MSOA21NM = "MSOA 2021 name"
  ),
  metadata = list(
    publisher = 'Office for National Statistics',
    date_published = '2023',
    dataset = 'Middle layer Super Output Areas (December 2021) Boundaries EW BGC (V2',
    url = 'https://geoportal.statistics.gov.uk/datasets/ed5c7b7d733d4fd582281f9bfc9f02a2_0',
    date_accessed = '16 October 2024',
    licence = 'Open Government Licence (UK)'
  ),
  output = "visualisation/derived_data/FlatGeobufs/GreaterManchester_MSOA_ONS_2025.fgb"
)
```

#### Local Authority Districts (LAD)
```{r}
data$Manchester$areas[["LAD"]] = list(
  source="visualisation/external_data/UK/Office of National Statistics/Local_Authority_Districts_December_2022_UK_BGC_V2_-4517174194749745377.gpkg",
  description = "Local Authority Districts (2022)",
  variable = list(
    LAD22CD = "LAD 2022 code",
    LAD22NM = "LAD 2022 name"
  ),
  metadata = list(
    publisher = 'Office for National Statistics',
    date_published = '2023',
    dataset = 'Local Authority Districts (December 2022) Boundaries UK BGC',
    url = 'https://geoportal.statistics.gov.uk/datasets/995533eee7e44848bf4e663498634849_0',
    date_accessed = '16 October 2024',
    licence = 'Open Government Licence (UK)'
  ),
  output = "visualisation/derived_data/FlatGeobufs/GreaterManchester_LAD_ONS_2025.fgb"
)
```

#### Greater Manchester
Greater Manchester is a ceremonial county; documentation on ceremonial counties is included in the Ordnance Survey Boundary-Line geopackage download, specified below.

```{r}
data$Manchester$areas[["GreaterManchester"]] = list(
  source="visualisation/external_data/UK/Ordnance Survey/bdline_gpkg_gb/Data/bdline_gb.gpkg",
  layer = "boundary_line_ceremonial_counties",
  filter = "Name == 'Greater Manchester'",
  description = "Greater Manchester",
  variable = list(
    Name = 'Name'
  ),
  metadata = list(
    publisher = 'Ordnance Survey',
    date_published = '2024',
    dataset = 'Boundary-Line™',
    url = 'https://osdatahub.os.uk/downloads/open/BoundaryLine',
    date_accessed = '11 October 2024',
    licence = 'Open Government Licence (UK)',
    notes = "bdline_gpkg_gb/Data/bdline_gb.gpkg|layername=boundary_line_ceremonial_counties|subset=\"Name\" = 'Greater Manchester'"
  ),
  output = "visualisation/derived_data/FlatGeobufs/GreaterManchester_OrdnanceSurvey_2025.fgb"
)
```


#### LSOA 2011 to 2021 look up table
The Index of Multiple Deprivation is only available for 2011 LSOAs, but we can use a look up table to approximately allocate this to the 2021 LSOAs.
```{r}

data$Manchester$linkage[['LSOA2011']] <- list(
  source="visualisation/external_data/UK/Office of National Statistics/LSOA_(2011)_to_LSOA_(2021)_to_Local_Authority_District_(2022)_Best_Fit_Lookup_for_EW_(V2).csv",
  description = "LSOA (2011) to LSOA (2021) to Local Authority District (2022) Best Fit Lookup for EW (V2)",
  variable = list(
    LSOA11CD = 'LSOA 2011 code',
    LSOA21CD = 'LSOA 2021 code'
  ),
  metadata = list(
    publisher = 'Office for National Statistics',
    date_published = '2023',
    dataset = 'LSOA (2011) to LSOA (2021) to Local Authority District (2022) Best Fit Lookup for EW (V2)',
    url = 'https://geoportal.statistics.gov.uk/datasets/ons::lsoa-2011-to-lsoa-2021-to-local-authority-district-2022-best-fit-lookup-for-ew-v2/about',
    date_accessed = '29 October 2024',
    licence = 'Open Government Licence (UK)'
  )
)
```

#### Index of Multiple Deprivation
```{r}

data$Manchester$linkage[['IMD']] <- list(
  source="visualisation/external_data/UK/Office of National Statistics/Index_of_Multiple_Deprivation_(Dec_2019)_Lookup_in_England.csv",
  description = "Index of Multiple Deprivation (December 2019) Lookup in EN.  The Index of Multiple Deprivation ranks every small area in England from 1 (most deprived area) to 32,844 (least deprived area).",
  variable = list(
    LSOA11CD = 'LSOA 2011 code',
    IMD19 = 'Index of Multiple Deprivation (December 2019)'
  ),
  metadata = list(
    publisher = 'Office for National Statistics',
    date_published = '2022',
    dataset = 'Index of Multiple Deprivation (December 2019) Lookup in EN',
    url = 'https://geoportal.statistics.gov.uk/datasets/ons::index-of-multiple-deprivation-december-2019-lookup-in-en/about',
    date_accessed = '29 October 2024',
    licence = 'Open Government Licence (UK)'
  )
)
```

### Melbourne areas 

#### Greater Melbourne

```{r}
data$Melbourne$areas[["GreaterMelbourne"]] = list(
  source="visualisation/external_data/Australia/Australian Bureau of Statistics/1270055001_ASGS_2016_vol_1_geopackage/ASGS 2016 Volume 1.gpkg",
  layer = "GCCSA_2016_AUST",
  filter = "GCCSA_NAME_2016 == 'Greater Melbourne'",
  description = "Greater Melbourne (Greater Capital City Statistical Area)",
  variable = list(
    GCCSA_CODE_2016 = 'Greater Capital City Statistical Area 2016 code',
    GCCSA_NAME_2016 = 'Greater Capital City Statistical Area 2016 name',
    STATE_CODE_2016 = 'State/Territory 2016 code',
    STATE_NAME_2016 = 'State/Territory 2016 name'
  ),
  metadata = list(
    publisher = 'Australian Bureau of Statistics',
    date_published = '2016',
    dataset = 'Greater Capital City Statistical Areas ASGS Edition 2',
    url = 'https://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.001July%202016',
    date_accessed = '16 December 2025',
    licence = 'Creative Commons Attribution 4.0 International licence (CC BY 4.0)',
    notes = "Filter for GCC_NAME16 = 'Greater Melbourne'"
  ),
  filter = "GCC_NAME16 == 'Greater Melbourne'",
  output = "visualisation/derived_data/FlatGeobufs/GreaterMelbourne_ABS_2025.fgb"
)
```

#### Mesh Block (MB)

Mesh Blocks are the smallest geographic area defined by the Australian Bureau of Statistics (ABS), and will be used for aligning correspondence between SA1s and LGA(s).

```{r}
data$Melbourne$areas[["SA1"]] = list(
  source="visualisation/external_data/Australia/Australian Bureau of Statistics/1270055001_ASGS_2016_vol_1_geopackage/ASGS 2016 Volume 1.gpkg",
  layer = "MB_2016_AUST",
  filter = "GCCSA_NAME_2016 == 'Greater Melbourne'",
  description = "Mesh Block (2016)",
  variable = list(
    MB_CODE_2016 = 'Mesh Block 2016 code',
    SA1_MAINCODE_2016 = 'SA1 2016 maincode',
    SA1_7DIGITCODE_2016 = 'SA1 2016 7-digit code',
    SA2_MAINCODE_2016 = 'SA2 2016 maincode',
    SA2_5DIGITCODE_2016 = 'SA2 2016 code',
    SA2_NAME_2016 = 'SA2 2016 name',
    SA3_CODE_2016 = 'SA3 2016 code',
    SA3_NAME_2016 = 'SA3 2016 name',
    SA4_CODE_2016 = 'SA4 2016 code',
    SA4_NAME_2016 = 'SA4 2016 name',
    GCCSA_NAME_2016 = 'Greater Capital City Statistical Area 2016 name',
    AREA_ALBERS_SQKM = 'Area in square kilometres (Albers)'
  ),
  metadata = list(
    publisher = 'Australian Bureau of Statistics',
    date_published = '2016',
    dataset = 'Statistical Area Level 1 (SA1) ASGS Edition 2',
    url = 'https://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.001July%202016',
    date_accessed = '16 December 2025',
    licence = 'Creative Commons Attribution 4.0 International licence (CC BY 4.0)'
  ),
  output = "visualisation/derived_data/FlatGeobufs/Melbourne_SA1_ABS_2025.fgb"
)
```

#### Statistical Area 1 (SA1)

```{r}
data$Melbourne$areas[["SA1"]] = list(
  source="visualisation/external_data/Australia/Australian Bureau of Statistics/1270055001_ASGS_2016_vol_1_geopackage/ASGS 2016 Volume 1.gpkg",
  layer = "SA1_2016_AUST",
  filter = "GCCSA_NAME_2016 == 'Greater Melbourne'",
  description = "Statistical Area Level 1 (2016)",
  variable = list(
    SA1_MAINCODE_2016 = 'SA1 2016 maincode',
    SA1_7DIGITCODE_2016 = 'SA1 2016 7-digit code',
    SA2_MAINCODE_2016 = 'SA2 2016 maincode',
    SA2_5DIGITCODE_2016 = 'SA2 2016 code',
    SA2_NAME_2016 = 'SA2 2016 name',
    SA3_CODE_2016 = 'SA3 2016 code',
    SA3_NAME_2016 = 'SA3 2016 name',
    SA4_CODE_2016 = 'SA4 2016 code',
    SA4_NAME_2016 = 'SA4 2016 name',
    GCCSA_NAME_2016 = 'Greater Capital City Statistical Area 2016 name',
    STATE_NAME_2016 = 'State/Territory 2016 name',
    AREA_ALBERS_SQKM = 'Area in square kilometres (Albers)'
  ),
  metadata = list(
    publisher = 'Australian Bureau of Statistics',
    date_published = '2016',
    dataset = 'Statistical Area Level 1 (SA1) ASGS Edition 2',
    url = 'https://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.001July%202016',
    date_accessed = '16 December 2025',
    licence = 'Creative Commons Attribution 4.0 International licence (CC BY 4.0)'
  ),
  output = "visualisation/derived_data/FlatGeobufs/Melbourne_SA1_ABS_2025.fgb"
)
```

#### Statistical Area 2 (SA2)

```{r}
data$Melbourne$areas[["SA2"]] = list(
  source="visualisation/external_data/Australia/Australian Bureau of Statistics/1270055001_ASGS_2016_vol_1_geopackage/ASGS 2016 Volume 1.gpkg",
  layer = "SA2_2016_AUST",
  filter = "GCCSA_NAME_2016 == 'Greater Melbourne'",
  description = "Statistical Area Level 2 (2016)",
  variable = list(
    SA2_5DIGITCODE_2016 = 'SA2 2016 code',
    SA2_NAME_2016 = 'SA2 2016 name',
    SA3_CODE_2016 = 'SA3 2016 code',
    SA3_NAME_2016 = 'SA3 2016 name',
    SA4_CODE_2016 = 'SA4 2016 code',
    GCCSA_NAME_2016 = 'Greater Capital City Statistical Area 2016 name',
    STATE_NAME_2016 = 'State/Territory 2016 name',
    AREA_ALBERS_SQKM = 'Area in square kilometres (Albers)'
  ),
  metadata = list(
    publisher = 'Australian Bureau of Statistics',
    date_published = '2016',
    dataset = 'Statistical Area Level 2 (SA2) ASGS Edition 2',
    url = 'https://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.001July%202016',
    date_accessed = '16 December 2025',
    licence = 'Creative Commons Attribution 4.0 International licence (CC BY 4.0)'
  ),
  output = "visualisation/derived_data/FlatGeobufs/Melbourne_SA2_ABS_2025.fgb"
)
```

#### Local Government Areas (LGA)

```{r}
data$Melbourne$areas[["LGA"]] = list(
  source="visualisation/external_data/Australia/Australian Bureau of Statistics/1270055003_asgs_2016_vol_3_aust_gpkg/ASGS 2016 Volume 3.gpkg",
  description = "Local Government Areas (2016)",
  variable = list(
    LGA_CODE_2016 = 'LGA 2016 code',
    LGA_NAME_2016 = 'LGA 2016 name',
    STATE_NAME_2016 = 'State/Territory 2016 name',
    AREA_ALBERS_SQKM = 'Area in square kilometres (Albers)'
  ),
  metadata = list(
    publisher = 'Australian Bureau of Statistics',
    date_published = '2016',
    dataset = 'Local Government Areas ASGS Edition 2',
    url = 'https://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.003July%202016',
    date_accessed = '16 December 2025',
    licence = 'Creative Commons Attribution 4.0 International licence (CC BY 4.0)'
  ),
  output = "visualisation/derived_data/FlatGeobufs/Melbourne_LGA_ABS_2025.fgb"
)
```

#### Melbourne LGA linkage
```{r}
data$Melbourne$areas[["MB_LGA_linkage"]] = list(
  source="visualisation/external_data/Australia/Australian Bureau of Statistics/1270055003_lga_2016_vic_csv/LGA_2016_VIC.csv",
  description = "Mesh Block (2016) to Local Government Area linkage",
  variable = list(
    MB_CODE_2016 = "Mesh Block 2016 code",
    LGA_CODE_2016 = "Local Government Area 2016 code",
    LGA_NAME_2016 = "Local Government Area 2016 name"
  ),
  metadata = list(
    publisher = 'Australian Bureau of Statistics',
    date_published = '2016',
    dataset = 'Victoria Local Government Area ASGS Edition 2016 in .csv Format',
    url = 'https://www.abs.gov.au/AUSSTATS/subscriber.nsf/log?openagent&1270055003_lga_2016_vic_csv.zip&1270.0.55.003&Data%20Cubes&5564B60717E84DB5CA25833D000EACFF&0&July%202016&07.11.2018&Previous',
    date_accessed = '17 December 2025',
    licence = 'Creative Commons Attribution 4.0 International licence (CC BY 4.0)'
  )
)
```

#### SEIFA Index of Relative Socio-Economic Disadvantage

The Socio-Economic Indexes for Areas (SEIFA) Index of Relative Socio-Economic Disadvantage (IRSD) is available for SA1, SA2 and LGA geographies. The IRSD summarises variables related to disadvantage, with lower scores indicating greater disadvantage.

```{r}
data$Melbourne$linkage[['SEIFA_SA1']] <- list(
  source="visualisation/external_data/Australia/Australian Bureau of Statistics/2033055001 - sa1 indexes.xls",
  description = "SEIFA 2016 Index of Relative Socio-Economic Disadvantage for SA1. The IRSD summarises variables related to disadvantage, with scores ranging from low (greater disadvantage) to high (less disadvantage) for comparisons across SA1s within Australia.",
  variable = list(
    SA1_7DIG16 = 'SA1 2016 code',
    IRSD_SCORE = 'Index of Relative Socio-Economic Disadvantage score',
    IRSD_DECILE = 'IRSD decile (1 = most disadvantaged 10%, 10 = least disadvantaged 10%)',
    IRSD_PERCENTILE = 'IRSD percentile (1 = most disadvantaged, 100 = least disadvantaged)'
  ),
  metadata = list(
    publisher = 'Australian Bureau of Statistics',
    date_published = '2023',
    dataset = 'Socio-Economic Indexes for Areas (SEIFA), Australia, 2016',
    url = 'https://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/2033.0.55.0012016',
    date_accessed = '16 December 2025',
    licence = 'Creative Commons Attribution 4.0 International licence (CC BY 4.0)'
  )
)

data$Melbourne$linkage[['SEIFA_SA2']] <- list(
  source="visualisation/external_data/Australia/Australian Bureau of Statistics/2033055001 - sa2 indexes.xls",
  description = "SEIFA 2016 Index of Relative Socio-Economic Disadvantage for SA2. The IRSD summarises variables related to disadvantage, with scores ranging from low (greater disadvantage) to high (less disadvantage) for comparisons across SA2s within Australia.",
  variable = list(
    SA2_5DIG16 = 'SA2 2016 code',
    IRSD_SCORE = 'Index of Relative Socio-Economic Disadvantage score',
    IRSD_DECILE = 'IRSD decile (1 = most disadvantaged 10%, 10 = least disadvantaged 10%)',
    IRSD_PERCENTILE = 'IRSD percentile (1 = most disadvantaged, 100 = least disadvantaged)'
  ),
  metadata = list(
    publisher = 'Australian Bureau of Statistics',
    date_published = '2023',
    dataset = 'Socio-Economic Indexes for Areas (SEIFA), Australia, 2016',
    url = 'https://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/2033.0.55.0012016',
    date_accessed = '16 December 2025',
    licence = 'Creative Commons Attribution 4.0 International licence (CC BY 4.0)'
  )
)

data$Melbourne$linkage[['SEIFA_LGA']] <- list(
  source="visualisation/external_data/Australia/Australian Bureau of Statistics/2033055001 - lga indexes.xls",
  description = "SEIFA 2016 Index of Relative Socio-Economic Disadvantage for LGA. The IRSD summarises variables related to disadvantage, with scores ranging from low (greater disadvantage) to high (less disadvantage) for comparisons across LGAs within Australia.",
  variable = list(
    LGA_CODE16 = 'LGA 2016 code',
    IRSD_SCORE = 'Index of Relative Socio-Economic Disadvantage score',
    IRSD_DECILE = 'IRSD decile (1 = most disadvantaged 10%, 10 = least disadvantaged 10%)',
    IRSD_PERCENTILE = 'IRSD percentile (1 = most disadvantaged, 100 = least disadvantaged)'
  ),
  metadata = list(
    publisher = 'Australian Bureau of Statistics',
    date_published = '2023',
    dataset = 'Socio-Economic Indexes for Areas (SEIFA), Australia, 2016',
    url = 'https://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/2033.0.55.0012016',
    date_accessed = '16 December 2025',
    licence = 'Creative Commons Attribution 4.0 International licence (CC BY 4.0)'
  )
)
```

### Network

#### Cycling stress 

The following variables were supplied for all versions of the JIBE network to be used for visualisation (2024-10-18).  The variables car85PercSpeedKPH, bikeStressJct and walkStressJct were not recommended for us in Melbourne, due to challenges in sourcing appropriate data.
```{r}
network_variables <- list(
  edgeID = 'identifier – unique for link but same for both directions',
  name = 'road name',
  fwd = 'binary – forward or reverse direction',
  carSpeedLimitMPH = 'speed limit in mph',
  car85PercSpeedKPH = 'observed vehicle speeds in kph',
  aadt = 'total AADT in both directions – this is the variable used to estimate cycling stress',
  car = 'binary, whether cars allowed on link',
  bike = 'binary, whether bikes allowed on link. Always matches the walk variable, but in some cases cyclists must dismount',
  walk = 'binary, whether walking allowed on link)
  dismount (binary, whether cyclists must dismount on the link',
  gradient = 'slope in m / m',
  bikeProtectionType = 'level of cycle protection, from best to worst: KERBED (i.e., offroad), PROTECTED, LANE, MIXED)',
  VGVI = 'greenness visibility',
  freightPOIs = 'density of freight-related POIs, with 0 best and 1 worst. This can increase cycling stress by up to 20%',
  bikeStressDiscrete = 'bike stress using discrete classifications based on DfT guidance, from best to worst: GREEN, AMBER, RED',
  bikeStress = 'bike stress as a continuous variable based on above, with 0 best and 1 worst)',
  bikeStressJct = 'bike junction stress, with 0 best and 1 worst'
)
```


```{r}
data$Manchester[["network"]] <- list()
data$Melbourne[["network"]] <- list()
```

```{r}
data$Manchester$network[["reference"]] <- list(
  source="visualisation/network/net2way_manchester.gpkg",
  layer="links",
  description = "Manchester reference network"
)
data$Melbourne$network[["reference"]] <- list(
  source="visualisation/network/net2way_melbourne.gpkg",
  layer="links",
  description = "Melbourne network"
)
```

```{r}
data$Manchester$network[["intervention"]] <- list(
  source="visualisation/network/net2way_manchester_cycleIntervention.gpkg",
  layer="links",
  description = "Manchester network with reduced speed limits and improved cycling infrastructure"
)
```

#### Volumes

Car/Bike/Pedestrian flow volumes have also been prepared on each link segment

```{r}
for (city in cities) {
  city_lower <- tolower(city)
  data[[city]][["volumes"]] <- list()
  for (scenario in data[[city]]$parameters$scenarios) {
      data[[city]]$volumes[[scenario]] <-list()
      data[[city]]$volumes[[scenario]][["bikePed"]] <- list()
      data[[city]]$volumes[[scenario]][["carTruck"]] <- list()
      for (day in c('saturday','sunday','thursday')) {
          data[[city]]$volumes[[scenario]]$bikePed[[day]] <- list(
            source = paste0(data[[city]]$parameters$results_folder,scenario,'/matsim/',data[[city]]$parameters$baseyear,'hourlyVolume_bikePed_',day,'.csv'),
            description = "These files contain modelled daily traffic counts by mode and hour for each day of the week. Number of records equal to number of links/edges in the network.",
            citation = "?",
            variables = list(
              linkId = 'link segment id by direction (out/rtn)',
              edgeId = 'link segement id both direction',
              osmId = 'open street map id',
              bike = 'Number of cyclists that travel past the link during that hour on an average day',
              ped = 'Number of pedestrian that travel past the link during that hour on an average day'
            ),
            rename = list(
            )
        )
        data[[city]]$volumes[[scenario]][["carTruck"]][[day]] <- list(
            source = paste0(data[[city]]$parameters$results_folder,scenario,'/matsim/',data[[city]]$parameters$baseyear,'hourlyVolume_carTruck_',day,'.csv'),
            description = "These files contain modelled daily traffic counts by mode and hour for each day of the week. Number of records equal to number of links/edges in the network.",
            citation = "?",
            variables = list(
              linkId = 'link segment id by direction (out/rtn)',
              edgeId = 'link segement id both direction',
              osmId = 'open street map id',
              car = 'Number of cars that travel past the link during that hour on an average day',
              truck = 'Number of trucks (Hgv) that travel past the link during that hour on an average day'
            ),
            rename = list(
            )
        )
      }
    }
}
```

#### Emissions

Link emissions may be available later, however these are not ready in the processed format as of 31 October 2024.
```{r}
for (city in cities) {
  city_lower <- tolower(city)
  data[[city]][["emissions"]] <- list()
  for (scenario in data[[city]]$parameters$scenarios) {
    for (day in c('saturday','sunday','thursday')) {
      data[[city]]$emissions[[scenario]] <- list(
          paste0(data[[city]]$parameters$results_folder,scenario,'/linkConcentration_',day,'.csv'),
          description = "These files contain modelled vehicle emissions on link segment by day of week. Number of records equal to number of links/edges in the network.",
          citation = "?",
          variables = list(
          ),
          rename = list(
          )
      )
    }
  }
}
```


### Synthetic population
```{r}
for (city in cities) {
  data[[city]][["population"]] <- list()
}
```

#### Persons
```{r}
for (city in cities) {
  for (scenario in data[[city]]$parameters$scenarios) {
    data[[city]]$population[[paste0("persons.",scenario)]] <- list(
      source = paste0(data[[city]]$parameters$results_folder,'microdata/pp_',data[[city]]$parameters$baseyear,'.csv'),
      description = paste0("A generated representation of ", city, "'s population characteristics, risks and outcomes (synthetic population)."),
      citation = "JIBE Project 2025",
      variables = list(
        personid = "person id", # renamed from id
        hhid = "household id",
        age = "in years",
        gender = "male (1), female (2)",
        occupation = "toddler (0), student (3), employed (1), unemployed (2), retiree (4)",
        workplace = "job id (0 if person is not employed)",
        income = paste0("in ", data[[city]]$parameters$currency),  # City-dependent currency
        mmetHr_walk = "Walking marginal metabolic equivalent task hours per week (mMET-h/wk)",
        mmetHr_cycle = "Cycling mMET (h/wk)",
        mmetHr_otherSport = "Other/sport mMET (h/wk)",
        exposure_normalised_pm25 = "PM2.5 exposure (µg/m³)",
        exposure_normalised_no2 = "NO₂ exposure (µg/m³)"
      ),
      rename = list(
        'id'= 'personId'
      )
    )
  }
}
```

#### Households
Dwelling ID (dwelling) and Household ID (id; omitted) appear identical (assert id==dwelling)
```{r}
for (city in cities) {
  city_lower <- tolower(city)
  data[[city]]$population[["households"]] <- list(
    source = paste0(city_lower,'/microData/hh_',data[[city]]$parameters$baseyear,'.csv'),
    description = paste0("A generated representation of ", city, " household characteristics, risks and outcomes (synthetic population)."),
    citation = "?",
    variables = list(
      hhid = "Household id",
      zone = "Output Area",
      hhSize = "Number of persons in household",
      autos = "Number of cars in household (0, 1, 2, >3)"
    ),
    rename = list(
      'id'= 'hhid',
      'zone' = 'zone_hh'
    )
  )
}
```

#### Dwellings
As per investigation further below, dwelling ID are household ID are identical in households dataset, but not in this data (assert id==hhID).  To link with households, hhID must be used.  Will omit dwelling coordinates from merged data.
```{r}
for (city in cities) {
  city_lower <- tolower(city)
  data[[city]]$population[["dwellings"]] <- list(
    source = paste0(city_lower,'/microData/dd_',data[[city]]$parameters$baseyear,'.csv'),
    description = paste0("A generated representation of ", city, "'s population characteristics, risks and outcomes (synthetic population)."),
    citation = "?",
    variables = list(
      hhid = "household id", #renamed for consistency
      zone = "Output Area",
      dw_type = "Dwelling type ((SFD: single-family detached, SFA: single-family attached, MF234: building with 2 to 4 units, MF5plus: building with 5 or more units)",
      bedrooms = "Number of bedrooms",
      quality = "Dwelling quality (1 to 4, being 1 top quality)",
      monthlyCost = paste0("Rent (", data[[city]]$parameters$currency, "/month)"),  # City-dependent currency
      yearBuilt = "Construction year",
      floor = "Floor area (m²)"
    ),
    rename = list(
      'zone' = 'zone_dw',
      'id' = 'dwid',
      'type' = 'dw_type'
    )
  )
}
```

#### Jobs
```{r}
for (city in cities) {
  city_lower <- tolower(city)
  data[[city]]$population[["jobs"]] <- list(
    source = paste0(city_lower,'/microData/jj_',data[[city]]$parameters$baseyear,'.csv'),
    description = paste0("A generated representation of ", city, "'s employment characteristics, risks and outcomes (synthetic population)."),
    citation = "?",
    variables = list(
      jobid = "job id",
      zone = "Output Area",
      personId = "person id",  # renamed for consistency
      job_type = "job type by industry"
    ),
    rename = list(
      'id' = 'jobid',
      'zone' = 'zone_jj',
      'type' = 'job_type'
    )
  )
}
```

### Trips

Potential use for visualization:

- Origin-Destination flow by mode
- Mode share by zones
- Distribution of distance kilometer walked/cycled/drived

```{r}
for (city in cities) {
  for (scenario in data[[city]]$parameters$scenarios) {
    data[[city]][["trips"]] <- list(
        source = paste0(data[[city]]$parameters$results_folder,scenario,data[[city]]$parameters$trips),
        description = "This file contains all the trip records over a week generated from travel demand model MITO. The total number of trip records is around 23 million.",
        citation = "JIBE Project 2025",
        variables = list(
          origin = 'Origin ID for linkage with zone geometry',
          destination = 'Destination ID for linkage with zone geometry',
          purpose = 'HBW (Home-based-work), HBE (Home-based-education), HBA (Home-based-accompanying), HBS (Home-based-shopping), HBR (Home-based-recreation), HBO (Home-based other), NHBO (non-home-based other), NHBW (non-home-based-work)',
          distance_walk = 'Walk distance from origin zone to destination zone [km]',
          distance_bike = 'Cycle distance from origin zone to destination zone [km]',
          distance_auto = 'Car distance from origin zone to destination zone [km]',
          time_auto = 'Car travel time from origin zone to destination zone [minute]',
          time_pt = 'public transport travel time from origin zone to destination zone [minute]',
          mode = 'autoDriver, autoPassenger, pt, walk, bicycle'
        ),
        rename = list(
        )
    )
  }
}
```

## Processing
### Network

Network data in the JIBE project is produced through an iterative agent-based modelling approach according to the JIBE model above.  Broadly, a range of exposures are linked to the network and then their influence on travel behaviours evaluated in conjunction with travel population survey data evaluated using the MITO (Microscopic Transportation Orchestrator; for travel demand) and SILO (Simple Integrated Land- Use Orchestrator; for land use) frameworks and MatSIM simulation software.  As such, different network data may be generated under different scenario conditions.  This can be used to evaluate the spatial and population change in exposures and outcomes resulting from specific urban planning interventions (e.g. low traffic speed zones, and investment in cycling infrastructure).

Network data used in MatSIM may be bi-directional, and when visualising this consideration must be given to how this will be represented and structured as data.  While attributes on network segments may vary by direction, typically the geometries do not (in fact, they are duplicated).  This means that a dataset's size may be significantly reduced by only including the single set of geometries and retaining the relevant variables (potentially having inbound and outbound variants).  Where there is difference in inbound and outbound routes, or it could be anticipated, there are options for representing this: there could be a systematic offset to represent the same network geometries side by side, with colour variation based on directional attributes; or, a function could be applied (e.g. the 'worst' statistic could be displayed). Alternately, another way of reducing data size is to remove all variables except the most important (e.g. traffic stress) and an ID variable that could be used on click to retrieve other relevant data as required.

For a preliminary visualisation of the network data, the following is proposed:

1. Only retain the unique edgeID containing the bikeStress rating

2. Re-project to WGS84 (EPSG 4326)

3. Export only relevant attributes for display

This should be repeated for both reference and intervention for all cities.

#### Process network data
```{r}
#| output: true

# Initialize parent city object
city_data <- list()

for (city in cities) {
  city_data[[city]] <- list()
  city_data[[city]]$networks <- list()
  
  for (network in data[[city]]$parameters$scenarios) {
    # Skip if network not defined for this city
    if (is.null(data[[city]]$network[[network]])) {
      next
    }
    
    city_data[[city]]$networks[[network]] <- st_read(
      paste0(data[[city]]$network[[network]]$source
      ), 
      layer = data[[city]]$network[[network]]$layer
    )
    
    city_data[[city]]$networks[[network]] %>% summary()
    # Number of edges in each direction
    city_data[[city]]$networks[[network]]$fwd %>% table()
    city_data[[city]]$networks[[network]] <- city_data[[city]]$networks[[network]]  %>%
      group_by(edgeID) %>%
      filter(bikeStress == max(bikeStress, na.rm = TRUE) | is.na(bikeStress)) %>%
      slice(1) %>%
      ungroup() %>%
      st_transform(city_data[[city]]$networks[[network]], crs = 4326)
    # Confirm non-duplicated count
    city_data[[city]]$networks[[network]]  %>% duplicated() %>% table()
  }
}
```

Trip volume data data for bicycles, walking, cars and trucks has been prepared for edge segments both for the reference case and cycling intervention scenario.  CSV files have been produced with results for a representative weekday (Thursday) as well as the two weekend days (Saturday and Sunday).  For simplicity I will only extract volume data for weekdays at this stage.

As above, reference and scenario data shall be loaded, then these four datasets will be linked together so there will be reference and scenario estimates for each of weekday volumes (bike, pedestrian, car) and stress.  That totals 8 variables, in addition to indices.

That is the theory, however the cycling intervention scenario results aren't yet in the accessible CSV format, so will hold off handling those until available.

For now, only weekday volumes for reference scenario will be loaded and linked.

The volume data are bidirectional, however the current display is unidirectional

```{r}
# Link datasets with volumes, only for thursday reference scenario
day <- 'thursday'
for (city in cities) {
  city_data[[city]]$volumes <- list()
  
  for (scenario in data[[city]]$parameters$scenarios) {
    # Skip if volumes not defined for this city/scenario
    if (is.null(data[[city]]$volumes[[scenario]])) {
      next
    }
    
    city_data[[city]]$volumes[[scenario]] <- list()
    for (mode in c('bikePed','carTruck')) {
      file <- paste0(base_path,
          data[[city]]$volumes[[scenario]][[mode]][[day]]$source
        )
      cat(file)
      city_data[[city]]$volumes[[scenario]][[mode]] <- read_csv(file)
      columns <- city_data[[city]]$volumes[[scenario]][[mode]] %>% names()
      counts <- setdiff(columns, c('linkId','edgeId','osmId'))
      city_data[[city]]$volumes[[scenario]][[mode]] <- city_data[[city]]$volumes[[scenario]][[mode]] %>%
        group_by(edgeId) %>%
        summarize(across(all_of(counts), \(x) sum(x, na.rm = TRUE), .names = "volume_{.col}"))
    }
  }
}

```

```{r}
# Join the datasets on edgeID for each city
for (city in cities) {
  for (scenario in data[[city]]$parameters$scenarios) {
    city_data[[city]]$networks[[scenario]] <- city_data[[city]]$networks[[scenario]] %>%
      left_join(
        city_data[[city]]$volumes[[scenario]][['bikePed']],
        by = c('edgeID' = 'edgeId')
      ) %>%
      left_join(
        city_data[[city]]$volumes[[scenario]][['carTruck']],
        by = c('edgeID' = 'edgeId')
      )
  }
  # join listed scenario specific network attributes into a single dataset with unique geometries
  # Start with the first scenario to get base geometry and shared attributes
  first_scenario <- data[[city]]$parameters$scenarios[1]
  city_data[[city]]$joined_networks <- city_data[[city]]$networks[[first_scenario]] %>% 
    select(edgeID, name) %>%
    as.data.frame()
  
  # Join scenario-specific attributes for each scenario
  for (scenario in data[[city]]$parameters$scenarios) {
    # Join network attributes (e.g., bikeStressDiscrete)
    scenario_network <- city_data[[city]]$networks[[scenario]] %>% 
      select(edgeID, bikeStressDiscrete) %>%
      as.data.frame()
    
    city_data[[city]]$joined_networks <- city_data[[city]]$joined_networks %>%
      left_join(
        scenario_network,
        by = "edgeID",
        suffix = c("", paste0("_", scenario))
      ) %>%
      rename_with(
        ~ if_else(.x == "bikeStressDiscrete", paste0("BSD_", scenario), .x)
      )
    
    # Join volume data if available
    if (!is.null(city_data[[city]]$volumes[[scenario]]$bikePed)) {
      city_data[[city]]$joined_networks <- city_data[[city]]$joined_networks %>%
        left_join(
          city_data[[city]]$volumes[[scenario]]$bikePed,
          by = c("edgeID" = "edgeId"),
          suffix = c("", paste0("_", scenario))
        )
    }
    
    if (!is.null(city_data[[city]]$volumes[[scenario]]$carTruck)) {
      city_data[[city]]$joined_networks <- city_data[[city]]$joined_networks %>%
        left_join(
          city_data[[city]]$volumes[[scenario]]$carTruck,
          by = c("edgeID" = "edgeId"),
          suffix = c("", paste0("_", scenario))
        )
    }
  }
  
  # Convert back to spatial object using geometry from first scenario
  geometry_col <- names(city_data[[city]]$networks[[first_scenario]])[
    sapply(city_data[[city]]$networks[[first_scenario]], inherits, "sfc")
  ][1]
  city_data[[city]]$joined_networks <- city_data[[city]]$joined_networks %>% 
    st_sf(sf_column_name = geometry_col)

}
```

Output network data:
```{r}
for (city in cities) {
  city_lower <- tolower(city)
  ## Prepare and join network data for each scenario
  derived_network_path <- paste0(base_path, "visualisation/derived_data/FlatGeobufs/", city_lower,"_4326.fgb")
  output_dir <- dirname(derived_network_path)
  if (!dir.exists(output_dir)) {
    dir.create(output_dir, recursive = TRUE)
  }
  
  if (file.exists(derived_network_path)) {
    file.remove(derived_network_path)
  }

  city_data[[city]]$joined_networks %>% 
    select(
      edgeID,
      name,
      starts_with("BSD_")
    ) %>%
    st_write(
      derived_network_path, 
      append = FALSE
    )
   
}
```

For now, this doesn't include all the upstream variables that relate to cycling stress.  Let's see how we can get this on the map using Tippecanoe to convert it to a pmtiles layer.  Later, we may be able to retrieve co-variates stored separately by querying a parquet file on a link-by-link basis.  This will save on data transfer and improve performance of the web app.

```{r}
for (city in cities) {
  city_lower <- tolower(city)
  derived_network_path <- paste0(base_path, "visualisation/derived_data/FlatGeobufs/", city_lower, "_network_bsd_scenarios_4326.fgb")
  pmtiles_network_path <- paste0(base_path, "visualisation/derived_data/PMTiles/", city_lower, "_network_bsd_scenarios_4326.pmtiles")
  
  if (file.exists(pmtiles_network_path)) {
    file.remove(pmtiles_network_path)
  }
  
  tippecanoe_command <- paste(
      "tippecanoe",
      "-o", 
      pmtiles_network_path,
      derived_network_path,
      "-l network -zg --no-feature-limit --no-tile-size-limit --force --read-parallel"
  )
  
  # Print the command to verify
  cat("Running command:", tippecanoe_command, "\n")
  
  # Run the tippecanoe command
  system(tippecanoe_command)
}
```

Now, let's also produce a more detailed parquet dataset of the reference and scenario attributes that may be queried for specific edge segments as required. 
```{r}
#| output: true
for (city in cities) {
  city_lower <- tolower(city)
  
  # Add a scenario column to each dataset
  for (scenario in data[[city]]$parameters$scenarios) {
    city_data[[city]]$networks[[scenario]] <- city_data[[city]]$networks[[scenario]] %>%
      mutate(scenario = scenario)
  }
  # Join all scenario datasets together based on the list of scenarios
  city_data[[city]]$full_network <- bind_rows(
    lapply(
      data[[city]]$parameters$scenarios,
      function(scenario) city_data[[city]]$networks[[scenario]] %>% as.data.frame()
    )
  )
  
  # Print the first few rows of the appended dataset
  print(paste("Network data for", city, ":"))
  print(head(city_data[[city]]$full_network))
}
```

```{r}
#| output: true
for (city in cities) {
  city_lower <- tolower(city)
  parquet_output_path <- paste0(base_path, "visualisation/derived_data/parquet/network_", city_lower, "_2021.parquet")
  output_dir <- dirname(parquet_output_path)
  if (!dir.exists(output_dir)) {
    dir.create(output_dir, recursive = TRUE)
  }
  write_parquet(city_data[[city]]$full_network %>% select(-path) %>% as.tibble(), parquet_output_path)
  cat("Merged network data written to Parquet file at", parquet_output_path, "\n")
}
```

Print out the variable names and types for reference:
```{r}
#| output: true
for (city in cities) {
  city_data[[city]]$full_network <- city_data[[city]]$full_network %>% select(-path)
  
  variables <- names(city_data[[city]]$full_network)
  data_types <- sapply(city_data[[city]]$full_network, class)
  
  formatted_variables <- glue::glue_data(
    tibble(variable = variables, data_type = data_types),
    "`{variable}` {data_type},"
  )
  cat(paste("\n", city, "network variables:\n"))
  cat(paste(formatted_variables, collapse = "\n"))
  cat("\n")
}
```

To create a parquet network table in AWS Athena, eg for Manchester

```{sql}
CREATE EXTERNAL TABLE IF NOT EXISTS `jibevisdatabase`.`network_manchester_2021` (
`edgeID` integer,
`osmID` integer,
`name` string,
`linkID` string,
`fwd` integer,
`length` double,
`cycleTime` double,
`walkTime` double,
`freespeed` double,
`carSpeedLimitMPH` double,
`car85PercSpeedKPH` double,
`width` double,
`lanes` integer,
`aadt` integer,
`aadtFwd` integer,
`aadtFwd_car` integer,
`aadtFwd_truck` integer,
`car` integer,
`bike` integer,
`walk` integer,
`motorway` integer,
`trunk` integer,
`primary` integer,
`dismount` integer,
`gradient` double,
`bikeProtectionType` string,
`endsAtJct` integer,
`crossesVehicles` integer,
`crossingTypeBike` string,
`crossingTypeWalk` string,
`crossingLanes` double,
`crossingWidth` double,
`crossingAADT` double,
`crossingSpeedLimit` double,
`crossing85PercSpeed` double,
`vgvi` double,
`freightPOIs` integer,
`bikeStressDiscrete` string,
`bikeStress` double,
`bikeStressJct` double,
`walkStressJct` double,
`scenario` string
)
COMMENT "Manchester network parquet"
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
LOCATION 's3://your-bucket/parquet/'
TBLPROPERTIES ('classification' = 'parquet');
```



### Population
```{r}
for (city in cities) {
  city_data[[city]]$synpop <- list()
  for (key in names(data[[city]]$population)) {
    city_data[[city]]$synpop[[key]] <- read_csv(paste0(base_path,data[[city]]$population[[key]]$source))
  }
}
```

Rename variables to avoid ambiguity and for consistency to simplify linkage:
```{r}
for (city in cities) {
  for (key in names(data[[city]]$population)) {
    if ('rename' %in% names(data[[city]]$population[[key]])) {
        cat(city, ":", key, "\n")
        rename_list <- setNames(
          names(data[[city]]$population[[key]]$rename),
          data[[city]]$population[[key]]$rename
        )
        # Rename the columns in synpop[[key]]
        city_data[[city]]$synpop[[key]] <- city_data[[city]]$synpop[[key]] %>% rename(!!!rename_list)
    }
  }
}
```

#### Join population data
First we want to vertically append all scenario data, with new fields identifying each scenario to support summary queries grouped by scenario.
```{r}
#| output: true
for (city in cities) {
  # Initialize list to store scenario datasets
  scenario_datasets <- list()
  
  # Add scenario column to each dataset and collect them
  for (scenario in data[[city]]$parameters$scenarios) {
    scenario_key <- paste0("persons.", scenario)
    if (!is.null(city_data[[city]]$synpop[[scenario_key]])) {
      scenario_datasets[[scenario]] <- city_data[[city]]$synpop[[scenario_key]] %>%
        mutate(scenario = scenario)
    }
  }
  
  # Append all scenario datasets
  city_data[[city]]$synpop$persons <- bind_rows(scenario_datasets) %>%
    as.data.frame()
  
  # Print the first few rows of the appended dataset
  print(paste("Population data for", city, ":"))
  print(head(city_data[[city]]$synpop$persons))
  print(paste("Scenarios included:", paste(unique(city_data[[city]]$synpop$persons$scenario), collapse = ", ")))
}
```


```{r}
for (city in cities) {
  city_data[[city]]$synpop[["merged"]] <- city_data[[city]]$synpop$persons %>%
     left_join(city_data[[city]]$synpop$jobs, by = "personId") %>%
     left_join(city_data[[city]]$synpop$households, by = "hhid") %>%
     left_join(city_data[[city]]$synpop$dwellings, by = c("hhid" = "hhID"))
}
```

Display a summary of the merged data
```{r}
#| output: true
for (city in cities) {
  print(paste("Summary for", city, ":"))
  print(city_data[[city]]$synpop$merged %>% summary(na.rm=False))
}
```

Confirm that the zone variable from households and dwellings are identical, as an extra check that this merge has worked as intended
```{r}
#| output: true
for (city in cities) {
  print(paste("Zone validation for", city, ":"))
  stopifnot(identical(city_data[[city]]$synpop$merged$zone_hh, city_data[[city]]$synpop$merged$zone_dw))
}
```

In the above data, I have renamed variables to ensure they are unique (e.g. 'type' for dwellings and jobs, respectively renamed to 'type_dw' and 'type_jobs'.  In this way, all the variables listed in the data dictionaries above reflect the variables post-renaming that may be exported in the joined dataset.

#### How to use the population data?

Because there are millions of records for each city, it will be inefficient to attach to store all these variables with geometries for querying.  A better approach is to use geometry zone id to retrieve and query the relevant subset of persons on click and display summary statistics.  

#### Link up merged data with linkage codes 

```{r}
#| output: true
for (city in cities) {
  # Get the appropriate area structure for this city
  if (city == 'Manchester') {
    oa_geoms <- st_read(paste0(base_path,data$Manchester$areas$OA$source))
    
    # Select the relevant attributes, omitting the geometries
    shp_attributes <- oa_geoms %>%
      st_set_geometry(NULL) %>%
      select(id, OA21CD, LSOA21CD, LSOA21NM) 
    
    # Perform a left join to merge the attributes with your existing data frame
    city_data[[city]]$synpop$merged <- (city_data[[city]]$synpop$merged %>%
      left_join(shp_attributes, by = c("zone_hh" = "id")) %>%
      left_join(shp_attributes, by = c("zone_jj" = "id")))
      
    city_data[[city]]$synpop$merged <- city_data[[city]]$synpop$merged %>%
      rename("OA21CD.home" = "OA21CD.x") %>%
      rename("LSOA21CD.home" = "LSOA21CD.x") %>%
      rename("OA21CD.job" = "OA21CD.y") %>%
      rename("LSOA21CD.job" = "LSOA21CD.y") %>%
      rename("LSOA21NM.home" = "LSOA21NM.x") %>%
      rename("LSOA21NM.job" = "LSOA21NM.y")
  } else if (city == 'Melbourne') {
    # Read SA1 geometries for Melbourne
    sa1_geoms <- st_read(paste0(base_path,data$Melbourne$areas$SA1$source), 
                         layer = data$Melbourne$areas$SA1$layer, 
                         quiet = TRUE) %>%
      filter(GCCSA_NAME_2016 == 'Greater Melbourne')
    
    # Select the relevant attributes from SA1 (includes SA2, SA3, SA4, LGA hierarchy)
    shp_attributes <- sa1_geoms %>%
      st_set_geometry(NULL) %>%
      select(SA1_7DIGITCODE_2016, SA1_MAINCODE_2016, SA2_5DIGITCODE_2016, SA2_NAME_2016,
             SA3_CODE_2016, SA3_NAME_2016, SA4_CODE_2016, SA4_NAME_2016) %>%
      mutate(zone_id = as.integer(SA1_MAINCODE_2016))
    
    # Perform left joins to merge the attributes for home and job zones
    city_data[[city]]$synpop$merged <- (city_data[[city]]$synpop$merged %>%
      left_join(shp_attributes, by = c("zone_hh" = "zone_id")) %>%
      left_join(shp_attributes, by = c("zone_jj" = "zone_id")))
      
    # Rename columns to distinguish home and job locations
    city_data[[city]]$synpop$merged <- city_data[[city]]$synpop$merged %>%
      rename("SA1_MAINCODE_2016.home" = "SA1_MAINCODE_2016.x") %>%
      rename("SA2_5DIGITCODE_2016.home" = "SA2_5DIGITCODE_2016.x") %>%
      rename("SA2_NAME_2016.home" = "SA2_NAME_2016.x") %>%
      rename("SA1_MAINCODE_2016.job" = "SA1_MAINCODE_2016.y") %>%
      rename("SA2_5DIGITCODE_2016.job" = "SA2_5DIGITCODE_2016.y") %>%
      rename("SA2_NAME_2016.job" = "SA2_NAME_2016.y") 
  }
  
  print(paste("Columns for", city, ":"))
  print(city_data[[city]]$synpop$merged %>% names())
}
```

### Areas

#### Read in area look up tables for linkage codes

SA1 LGA codes will be assigned based on largest cumulative mesh block area within each SA1 associated with a single LGA.

```{r}
#| output: true
for (city in cities) {
  if (city == 'Manchester') {
    oa_lookup <- read_csv(paste0(base_path,data$Manchester$areas$OA_linkage$source))
    oa_lookup_selected <- oa_lookup %>%
      select(OA21CD, MSOA21CD, MSOA21NM, LAD22CD, LAD22NM)
    
    city_data[[city]]$synpop$merged <- city_data[[city]]$synpop$merged %>%
      left_join(oa_lookup_selected, by = c("OA21CD.home"="OA21CD")) %>%
      left_join(oa_lookup_selected, by = c("OA21CD.job"="OA21CD")) %>%
      rename("MSOA21CD.home" = "MSOA21CD.x") %>%
      rename("LAD22CD.home" = "LAD22CD.x") %>%
      rename("MSOA21CD.job" = "MSOA21CD.y") %>%
      rename("LAD22CD.job" = "LAD22CD.y")
    
    city_data[[city]]$synpop$merged <- city_data[[city]]$synpop$merged %>%
      rename("MSOA21NM.home" = "MSOA21NM.x") %>%
      rename("LAD22NM.home" = "LAD22NM.x") %>%
      rename("MSOA21NM.job" = "MSOA21NM.y") %>%
      rename("LAD22NM.job" = "LAD22NM.y")
    
    print(paste("Columns for", city, "after area linkage:"))
    print(city_data[[city]]$synpop$merged %>% names())
  } else if (city == 'Melbourne') {
    # Read Mesh Block geometries for Melbourne (contains both SA1 and will link to LGA)
    mb_geoms <- st_read(paste0(base_path, data$Melbourne$areas$MB$source), 
                        layer = data$Melbourne$areas$MB$layer, 
                        quiet = TRUE) %>%
      filter(GCCSA_NAME_2016 == 'Greater Melbourne')
    
    # Select MB to SA1 correspondence with area
    mb_to_sa1 <- mb_geoms %>%
      st_set_geometry(NULL) %>%
      select(MB_CODE_2016, SA1_MAINCODE_2016, AREA_ALBERS_SQKM) %>%
      distinct()
    
    # Read LGA lookup (MB to LGA correspondence)
    lga_lookup <- read_csv(paste0(base_path, data$Melbourne$areas$MB_LGA_linkage$source), show_col_types = FALSE)
    
    # Join MB to LGA with MB to SA1 to create SA1 to LGA correspondence
    # Assign SA1 to LGA with largest cumulative mesh block area
    sa1_to_lga <- mb_to_sa1 %>%
      left_join(lga_lookup, by = "MB_CODE_2016") %>%
      # Sum area by SA1 and LGA combination
      group_by(SA1_MAINCODE_2016, LGA_CODE_2016, LGA_NAME_2016) %>%
      summarise(total_area_sqkm = sum(AREA_ALBERS_SQKM, na.rm = TRUE), .groups = 'drop') %>%
      # Take the LGA with the largest area for each SA1
      group_by(SA1_MAINCODE_2016) %>%
      slice_max(total_area_sqkm, n = 1, with_ties = FALSE) %>%
      ungroup() %>%
      select(SA1_MAINCODE_2016, LGA_CODE_2016, LGA_NAME_2016) %>%
      mutate(zone_id = as.integer(SA1_MAINCODE_2016))
    
    # Join LGA data to the merged synpop for home and job locations
    city_data[[city]]$synpop$merged <- city_data[[city]]$synpop$merged %>%
      left_join(sa1_to_lga %>% select(zone_id, LGA_CODE_2016, LGA_NAME_2016), 
                by = c("zone_hh" = "zone_id")) %>%
      left_join(sa1_to_lga %>% select(zone_id, LGA_CODE_2016, LGA_NAME_2016), 
                by = c("zone_jj" = "zone_id")) %>%
      rename("LGA_CODE_2016.home" = "LGA_CODE_2016.x") %>%
      rename("LGA_NAME_2016.home" = "LGA_NAME_2016.x") %>%
      rename("LGA_CODE_2016.job" = "LGA_CODE_2016.y") %>%
      rename("LGA_NAME_2016.job" = "LGA_NAME_2016.y")
    
    print(paste("Columns for", city, "after LGA linkage:"))
    print(city_data[[city]]$synpop$merged %>% names())
  }
}
```

#### Write population with linkage codes to Parquet

[Parquet](https://parquet.apache.org/docs/overview/) is an open source file format designed by Apache for efficient querying of large data, and so is a good fit for our synethetic population containing approximately 3 million records.  Potentially it could also be used to store and query extended network attributes, and reduce the file size of map tiles.

```{r}
#| output: true
for (city in cities) {
  city_lower <- tolower(city)
  parquet_output_path <- paste0(base_path, "visualisation/derived_data/parquet/synpop_", city_lower, "_2021.parquet")
  output_dir <- dirname(parquet_output_path)
  if (!dir.exists(output_dir)) {
    dir.create(output_dir, recursive = TRUE)
  }
  write_parquet(city_data[[city]]$synpop$merged, parquet_output_path)
  cat("Merged synthetic population data written to Parquet file at", parquet_output_path, "\n")
}
```

#### Create queryable database using Parquet file

There are number of ways of querying parquet files, one being using a service like [Amazon Athena](https://www.cloudforecast.io/blog/using-parquet-on-athena-to-save-money-on-aws/), that promises speedy and cost-efficient querying of large data sources stored in the Parquet format on Amazon S3 cloud storage.

In the below we define a table for the synthetic population on the Athena jibevisdatabase, that will support subsequent querying. For example, a user might click on an MSOA and 3D curved lines might be drawn connecting the home and work LSOA locations for synthetic residents, colourised by some relevant statistic, grouping or covariate.  Or interactive gender and age group stratified box  plots of a continuous variable like totalActivityTime or mmetHr_cycle could be displayed.  Or a scatter plot of totalTravelTime by totalActivity time.  And so forth.   Separating population statistics from the geometry could support development of post hoc interactive queries like these.

The steps are:

1. Upload the Parquet File to S3; I stored it in a sub-folder 'parquet'
2. Set up [Athena](https://console.aws.amazon.com/athena/home) 

We have set up an Athena database using AWS CDK:

https://github.com/jibeproject/jibe-vis/blob/4cf03e9b5eced3376ef3b1a5cca6101496a01fc6/app/amplify/backend.ts#L44-L73

We can now set up the S3 bucket parquet folder as the location for results, and create a table that we can subsequently query.  In principle this could be done using AWS CDK, however for now, it has been done in the AWS Athena console using the following SQL query:

```{sql}
CREATE EXTERNAL TABLE IF NOT EXISTS `jibevisdatabase`.`synpop_manchester_2021` (
  `personid` double COMMENT 'Unique person identifier',
  `hhid` double COMMENT 'Household identifier',
  `scenario` string COMMENT 'Reference or Cycling intervention scenario',
  `age` double COMMENT 'Age (years)',
  `gender` double COMMENT 'Gender',
  `occupation` double,
  `income` double,
  `mmethr_walk` double,
  `mmethr_cycle` double,
  `mmetHr_otherSport` double,
  `exposure_normalised_pm25` double,
  `exposure_normalised_no2` double,
  `jobid` double,
  `zone_jj` double,
  `job_type` string,
  `dwelling` double,
  `hhsize` double,
  `zone_hh` double,
  `autos` double,
  `dwid` double,
  `zone_dw` double,
  `dw_type` string,
  `bedrooms` double,
  `quality` double,
  `monthlycost` double,
  `yearbuilt` double,
  `floor` double,
  `OA21CD.home` string,
  `LSOA21CD.home` string,
  `OA21CD.job` string,
  `LSOA21CD.job` string,
  `MSOA21CD.home` string,
  `LAD22CD.home` string,
  `MSOA21CD.job` string,
  `LAD22CD.job` string
) COMMENT "A test table created using 'Create table from S3 bucket data' using AWS Athena"
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
LOCATION 's3://your-bucket/parquet/'
TBLPROPERTIES ('classification' = 'parquet');
```

Note that Athena will convert all variable names to lower case, and this must be accounted for in any queries.

3. Set up a Lambda function to query Athena

See https://github.com/jibeproject/jibe-vis/blob/main/app/amplify/lambda/athena-parquet-query/lambda_function.py

4. Set up 'on click' function to run the Lambda function to retrieve data using a query given an area ID and other related parameters.

See `queryJibeParquet()` and `linkageGraph()` functions in https://github.com/jibeproject/jibe-vis/blob/main/app/src/components/vis/graphs.tsx.

#### Convert areas to FlatGeobuf data

```{r}
#| output: true
for (city in cities) {
  # Process the main city boundary
  city_boundary_key <- data[[city]]$parameters$name %>% replace(" ", "")
  
  if (!is.null(data[[city]]$areas[[city_boundary_key]])) {
    area <- data[[city]]$areas[[city_boundary_key]]
    
    # Check if the output is specified and ends with .fgb
    if (!is.null(area$output) && grepl("\\.fgb$", area$output)) {
      # Define the geopackage path and output path
      in_path <- paste0(base_path, area$source)
      out_path <- paste0(base_path, area$output)
    
      if ('layer' %in% names(area)) {
        layer <- area$layer
      } else {
        layer <- NULL
      }
      if ('filter' %in% names(area)) {
        filter <- area$filter
      } else {
        filter <- NULL
      }
      
      # Apply the spatial_data_to_fgb function
      data[[city]]$areas[[city_boundary_key]][['data']] <- spatial_data_to_fgb(in_path, out_path, layer, filter)
    }
  }
}

```


The area data is detailed with many features; it's better to restrict to those areas referenced in the population dataset.

For Manchester LSOAs, we will also use this layer to depict 2011 Index of Multiple Deprivation (2019; IMD).  To do this, we need to join on the correspondence codes for 2021 to 2011 LSOAs, and then join on the IMD data.

```{r}
#| output: true
for (city in cities) {
  # Check if linkage_areas parameter is defined and not NULL for this city
  if (!is.null(data[[city]]$parameters$linkage_areas)) {
    # Get linkage areas configuration
    linkage_areas <- data[[city]]$parameters$linkage_areas
    
    # Loop over each area type
    for (area_type in names(linkage_areas)) {
      area_config <- linkage_areas[[area_type]]
      area_id <- area_config$id
      
      # Create a unique list of values for homes and jobs based on the area type
      filter_values <- unique(c(
        city_data[[city]]$synpop$merged[[paste0(area_id, '.home')]], 
        city_data[[city]]$synpop$merged[[paste0(area_id, '.job')]]
      ))
      
      # Format these values as a character vector within the filter string
      filter_string <- paste0(
        area_id,
        " %in% c('", 
        paste(filter_values, collapse = "','"),
        "')"
      )
      print(filter_string)
     
      # Apply the spatial_data_to_fgb function with the correct filter string
      spatial_data_to_fgb(
        paste0(base_path, data[[city]]$areas[[area_type]]$source),
        paste0(base_path, data[[city]]$areas[[area_type]]$output),
        filter_condition = filter_string,
        linkage = area_config$linkage,
        variables = data[[city]]$areas[[area_type]]$variables %>% names()
      )
    }
  } else {
    cat(paste("No linkage_areas configured for", city, "- skipping area processing\n"))
  }
}
```


#### Generate pmtiles file containing area layers

```{r}
#| output: true
for (city in cities) {
  city_lower <- tolower(city)
  output_pmtiles <- paste0(base_path, "visualisation/derived_data/PMTiles/", city, ".pmtiles")
  output_dir <- dirname(output_pmtiles)
  
  # Check if the directory exists, and if not, create it
  if (!dir.exists(output_dir)) {
    dir.create(output_dir, recursive = TRUE)
  }
  areas <- c()
  # Get the list of output fgb files
  for (area_name in names(data[[city]]$areas)) {
    area <- data[[city]]$areas[[area_name]]
    # Check if the output is specified and ends with .fgb
    if (!is.null(area$output) && is.character(area$output)) {
      areas <- areas %>% append(paste0(base_path, area$output))
    }
  }
  
  if (file.exists(output_pmtiles)) {
    file.remove(output_pmtiles)
  }
  
  tippecanoe_command <- paste(
      "tippecanoe",
      "-o", 
      output_pmtiles,
      paste(areas, collapse=" "),
      "-r1 --no-feature-limit --no-tile-size-limit --force --read-parallel --coalesce-densest-as-needed --extend-zooms-if-still-dropping"
  )
  
  # Print the command to verify
  cat("Running command:", tippecanoe_command, "\n")
  
  # Run the tippecanoe command
  system(tippecanoe_command)
}

```

### Trips

```{r}
#| output: true
for (city in cities) {
  city_lower <- tolower(city)
  city_data[[city]]$trips <- list()
  
  # Load trips data if available
  if (!is.null(data[[city]]$trips)) {
    city_data[[city]]$trips$reference <- read_csv(paste0(base_path,data[[city]]$trips$source))
    
    parquet_output_path <- paste0(base_path, "visualisation/derived_data/parquet/trips_", city_lower, ".parquet")
    output_dir <- dirname(parquet_output_path)
    if (!dir.exists(output_dir)) {
      dir.create(output_dir, recursive = TRUE)
    }
    write_parquet(city_data[[city]]$trips$reference, parquet_output_path)
    cat("Merged reference trips data written to Parquet file at", parquet_output_path, "\n")
  } else {
    cat(city, "trips data not yet available\n")
  }
}

```

Get the trips data types for Athena query
```{r}
#| output: true
for (city in cities) {
  if (!is.null(city_data[[city]]$trips$reference)) {
    variables <- names(city_data[[city]]$trips$reference)
    data_types <- sapply(city_data[[city]]$trips$reference, class)
    
    # Replace 'numeric' with 'double' and 'character' with 'string'
    data_types[data_types == "numeric"] <- "double"
    data_types[data_types == "character"] <- "string"
    
    # Create a formatted string for each variable and its data type
    formatted_variables <- paste0("`", variables, "` ", data_types, ",", collapse = "\n")
    
    # Print the formatted variables
    cat(paste("\n", city, "trips variables:\n"))
    cat(formatted_variables)
    cat("\n")
  }
}
```

AWS Athena query:
```{sql}
CREATE EXTERNAL TABLE IF NOT EXISTS `jibevisdatabase`.`trips_manchester` (
`hh.id` double,
`p.ID` double,
`t.id` double,
`origin` double,
`originX` double,
`originY` double,
`destination` double,
`destinationX` double,
`destinationY` double,
`t.purpose` string,
`t.distance_walk` double,
`t.distance_bike` double,
`t.distance_auto` double,
`time_auto` double,
`time_pt` string,
`cost_bike_commute` double,
`cost_bike_disc` double,
`cost_walk_commute` double,
`cost_walk_disc` double,
`mode` string,
`departure_day` string,
`departure_time` double,
`departure_time_return` double
) COMMENT "Manchester trips"
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
LOCATION 's3://your-bucket/parquet/'
TBLPROPERTIES ('classification' = 'parquet');
```

## Generating Basemaps

Basemaps for the Transport Health Impacts platform are generated and visualised based on OpenStreetMap data using the Protomaps [PMTiles](https://github.com/protomaps/PMTiles) specification and tools.

At time of writing, basemaps for selected areas can be generated locally (as described below), or downloaded using an interactive map from https://app.protomaps.com/.

To generate a baseman for relevant region locally
1. Get and install PMTiles

  - see directions at https://github.com/protomaps/PMTiles
  - for MacOSX with Home Brew: `brew install pmtiles` (as per https://formulae.brew.sh/formula/pmtiles)

2. Run command, e.g.

  - `pmtiles extract  https://build.protomaps.com/20240812.pmtiles manchester_2024-08-12.pmtiles --bbox=-3.2907,52.9202,-1.1804,54.0346`
	- `pmtiles extract  https://build.protomaps.com/20240812.pmtiles munich_2024-08-12.pmtiles --bbox=11.0439,47.8297,12.0714,48.4927`
	- `pmtiles extract https://build.protomaps.com/20240812.pmtiles victoria_2024-08-12.pmtiles --bbox=139.55,-39.42,150.46,-33.75`

3. Merge basemaps, optionally:

`tile-join -o jibe_basemap.pmtiles manchester_2024-08-12.pmtiles munich_2024-08-12.pmtiles victoria_2024-08-12.pmtiles`


Credit Protomaps and OpenStreetMap Contributors under ODbL, e.g.
[Protomaps](https://protomaps.com/) © [OpenStreetMap](https://openstreetmap.org/)


Generated basemap style following directions at https://github.com/protomaps/basemaps/tree/main/styles.  Copied the English style as 'protomaps-v4.json' and saved this in the map components folder.